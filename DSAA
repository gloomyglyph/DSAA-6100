{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":62898,"databundleVersionId":6849998,"sourceType":"competition"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Large Language Models with LoRA on Kaggle\r\n\r\nIn this notebook, we explore how to fine-tune a large language model (MT5) with a technique called Low-Rank Adaptation (LoRA). LoRA allows us to adapt large models without modifying all parameters, making it efficient and feasible for resource-constrained environments. This is especially useful on platforms like Kaggle where GPU and memory resources may be limited.\r\n\r\n**Objectives:**\r\n- Load and configure an MT5 model and tokenizer.\r\n- Preprocess data for training.\r\n- Apply LoRA for efficient fine-tuning.\r\n- Train, evaluate, and analyze the model's performance.\r\n\r\nLet's dive in!\r\n","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup\r\n\r\nFirst, we ensure that all necessary libraries are installed and loaded. This section will help set up libraries specific to Hugging Face's Transformers, dataset handling, and LoRA adaptation. If any packages are missing, we include helper functions to install them seamlessly.\r\n","metadata":{}},{"cell_type":"code","source":"import transformers\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer, Trainer, TrainingArguments\nimport torch\nimport os\nfrom datasets import Dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:33:54.635583Z","iopub.execute_input":"2024-11-05T08:33:54.636443Z","iopub.status.idle":"2024-11-05T08:34:14.062108Z","shell.execute_reply.started":"2024-11-05T08:33:54.636387Z","shell.execute_reply":"2024-11-05T08:34:14.061166Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dsaa-6100-finetune-llm/train_data.csv\n/kaggle/input/dsaa-6100-finetune-llm/test_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def install_package(package):\n    import subprocess \n    import sys\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package], stdout=open(os.devnull, 'wb'), stderr=open(os.devnull, 'wb'))\n\n# Example: Install numpy without logs\ninstall_package('peft')\n\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:14.063653Z","iopub.execute_input":"2024-11-05T08:34:14.064273Z","iopub.status.idle":"2024-11-05T08:34:26.063632Z","shell.execute_reply.started":"2024-11-05T08:34:14.064238Z","shell.execute_reply":"2024-11-05T08:34:26.062880Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU\")# Use GPU\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:26.064725Z","iopub.execute_input":"2024-11-05T08:34:26.065053Z","iopub.status.idle":"2024-11-05T08:34:26.103441Z","shell.execute_reply.started":"2024-11-05T08:34:26.065020Z","shell.execute_reply":"2024-11-05T08:34:26.102465Z"}},"outputs":[{"name":"stdout","text":"GPU\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Model and Tokenizer Initialization\r\n\r\nWe initialize the pre-trained MT5 model and its tokenizer. MT5 is a multilingual transformer model, particularly effective for tasks across different languages. This step prepares the model for adaptation and makes it ready for fine-tuning in the next steps.\r\n","metadata":{}},{"cell_type":"code","source":"# Load the model and tokenizer\nmodel_name = \"google/flan-t5-base\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:26.105658Z","iopub.execute_input":"2024-11-05T08:34:26.106010Z","iopub.status.idle":"2024-11-05T08:34:32.923955Z","shell.execute_reply.started":"2024-11-05T08:34:26.105966Z","shell.execute_reply":"2024-11-05T08:34:32.923053Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94896f32db5740e5984c199be8118a26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26e7723e9544fc08eae13ae68040519"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f0831db836459789c0416ac27001b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85b4cc3a25744418e38da638bfba63b"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bf08229bb642178fac0e69ca9c3885"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f1509eb9d0464dbc2ea6c52491cf0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d57f82cdd24076816700a7c9d6aad3"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Introduction to LoRA Fine-Tuning\r\n\r\nFine-tuning large models directly can be resource-intensive, as it requires updating all model parameters. LoRA (Low-Rank Adaptation) is a technique that allows us to add new weights to the model in a way that reduces the number of parameters requiring updates, achieving nearly equivalent performance with far fewer computations. Here, we set up LoRA configurations, specifying parameters like rank and scaling factors.\r\n","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n# Define LoRA configuration for PEFT\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,  # Set task type as sequence-to-sequence\n    r=16,  # LoRA rank, controls the number of adapter parameters\n    lora_alpha=32,  # Scaling factor for LoRA\n    lora_dropout=0.1,  # Dropout for regularization\n    target_modules=[\"q\", \"v\"],  # Apply LoRA to the query and value layers in attention\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:32.925236Z","iopub.execute_input":"2024-11-05T08:34:32.925532Z","iopub.status.idle":"2024-11-05T08:34:32.930471Z","shell.execute_reply.started":"2024-11-05T08:34:32.925499Z","shell.execute_reply":"2024-11-05T08:34:32.929622Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def report_trainable_parameters(model):\n    trainable_params = 0\n    total_params = 0\n    \n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            #print(f\"Trainable parameter: {name}\")\n            trainable_params += param.numel()\n        total_params += param.numel()\n    \n    print(f\"\\nTotal trainable parameters: {trainable_params}\")\n    print(f\"Total parameters: {total_params}\")\n    print(f\"Trainable parameters percentage : {100*trainable_params/total_params}\")\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:32.931602Z","iopub.execute_input":"2024-11-05T08:34:32.931912Z","iopub.status.idle":"2024-11-05T08:34:32.943942Z","shell.execute_reply.started":"2024-11-05T08:34:32.931879Z","shell.execute_reply":"2024-11-05T08:34:32.943091Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"report_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:32.945179Z","iopub.execute_input":"2024-11-05T08:34:32.945458Z","iopub.status.idle":"2024-11-05T08:34:32.956700Z","shell.execute_reply.started":"2024-11-05T08:34:32.945427Z","shell.execute_reply":"2024-11-05T08:34:32.955700Z"}},"outputs":[{"name":"stdout","text":"\nTotal trainable parameters: 247577856\nTotal parameters: 247577856\nTrainable parameters percentage : 100.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Apply LoRA to the mT5 model\nmodel = get_peft_model(model, lora_config)\nreport_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:32.957831Z","iopub.execute_input":"2024-11-05T08:34:32.958140Z","iopub.status.idle":"2024-11-05T08:34:33.097203Z","shell.execute_reply.started":"2024-11-05T08:34:32.958107Z","shell.execute_reply":"2024-11-05T08:34:33.096365Z"}},"outputs":[{"name":"stdout","text":"\nTotal trainable parameters: 1769472\nTotal parameters: 249347328\nTrainable parameters percentage : 0.7096414524241463\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Data Preprocessing\r\n\r\nPreparing our dataset is critical for effective model training. In this section, we load and preprocess the data, converting it into a format compatible with the MT5 model. This includes tokenizing text inputs and structuring them into a dataset suitable for conditional generation tasks.\r\n","metadata":{}},{"cell_type":"code","source":"# Load training and testing data\ntrain_data = pd.read_csv('/kaggle/input/dsaa-6100-finetune-llm/train_data.csv')\ntest_data = pd.read_csv('/kaggle/input/dsaa-6100-finetune-llm/test_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:33.098385Z","iopub.execute_input":"2024-11-05T08:34:33.099023Z","iopub.status.idle":"2024-11-05T08:34:33.159532Z","shell.execute_reply.started":"2024-11-05T08:34:33.098979Z","shell.execute_reply":"2024-11-05T08:34:33.158592Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_data.iloc[0]['Input Context']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:33.163766Z","iopub.execute_input":"2024-11-05T08:34:33.164423Z","iopub.status.idle":"2024-11-05T08:34:33.174160Z","shell.execute_reply.started":"2024-11-05T08:34:33.164389Z","shell.execute_reply":"2024-11-05T08:34:33.173267Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] What are the differences between the dreambooth and textual inversion techniques to customize and enhance a diffusion generative model? \\nCould you provide examples of use-cases when one could be better than the other? </INST>'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train_data.iloc[0]['Output Content']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:33.175255Z","iopub.execute_input":"2024-11-05T08:34:33.175538Z","iopub.status.idle":"2024-11-05T08:34:33.184874Z","shell.execute_reply.started":"2024-11-05T08:34:33.175507Z","shell.execute_reply":"2024-11-05T08:34:33.184033Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"' Dreambooth and Textual Inversion are two techniques for customizing and enhancing a Diffusion Generative Model (DGM).\\n\\nDreambooth refers to a technique where the DGM is fine-tuned to generate high-quality images that match a given description, by using a discriminator network to evaluate the quality of the generated images and to guide the training process. The dreambooth technique is particularly useful in use-cases where the goal is to generate images based on textual descriptions, such as in computer vision or artistic applications.\\n\\nTextual Inversion refers to a technique where the DGM is fine-tuned to generate textual descriptions that match a given image, by using a discriminator network to evaluate the quality of the generated descriptions and to guide the training process. The textual inversion technique is particularly useful in use-cases where the goal is to generate descriptive texts for images, such as in natural language processing or image captioning applications.\\n\\nIn summary, both dreambooth and textual inversion techniques can be used to enhance the performance of a DGM by fine-tuning it to specific use-cases, but they focus on different aspects of the model - dreambooth on image generation and textual inversion on textual description generation. The choice between the two techniques would depend on the specific requirements of the application. </s><s>[INST] very interesting. what else can Textual Inversion be used for? '"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_data.iloc[330]['Input Context']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:33.186593Z","iopub.execute_input":"2024-11-05T08:34:33.187025Z","iopub.status.idle":"2024-11-05T08:34:33.195138Z","shell.execute_reply.started":"2024-11-05T08:34:33.186991Z","shell.execute_reply":"2024-11-05T08:34:33.194296Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] can you layout base cabinets for me for a 12 x 12 L shaped kitchen with the sink centered on the right and stove and fridge of the left? Use just standard RTA cabinet sizes. </INST>'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_data.iloc[330]['Output Content']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:34:33.196611Z","iopub.execute_input":"2024-11-05T08:34:33.197067Z","iopub.status.idle":"2024-11-05T08:34:33.204730Z","shell.execute_reply.started":"2024-11-05T08:34:33.197024Z","shell.execute_reply":"2024-11-05T08:34:33.203869Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"' Sure, here\\'s a layout for the base cabinets in a 12 x 12 L-shaped kitchen with a sink on the right and the stove and refrigerator on the left, using standard RTA cabinet sizes:\\n\\n| 36\" Sink Base | 30\" Corner Base | 24\" Base | 36\" Base |\\n| 30\" Base     |                |           | 24\" Base |\\n\\nPlease keep in mind that this is just one possible layout; there are many more depending on your specific needs and preferences. You should also consider each cabinet\\'s height, depth, and width to ensure that it fits properly in your kitchen space and provides the necessary storage and functionality. To design a kitchen layout that meets your needs and fits your space, it\\'s always a good idea to consult with a professional or use an online planning tool. </s>'"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"We see that the dataset already includes special tokens like [INST]. We can either remove them before passing it to the tokenizer or inform the tokenizer that the input text includes special characters","metadata":{}},{"cell_type":"code","source":"# We should remove special charachters from the dataset like [INST], <s>, ...\ndef remove_special_charachters(input_text):\n    cleaned_text = input_text.replace(\"<s>\", \"\").replace(\"[INST]\", \"\").replace(\"</INST>\", \"\").replace(\"</s>\", \"\").strip()\n    return cleaned_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:35:09.706475Z","iopub.execute_input":"2024-11-05T08:35:09.707483Z","iopub.status.idle":"2024-11-05T08:35:09.715115Z","shell.execute_reply.started":"2024-11-05T08:35:09.707421Z","shell.execute_reply":"2024-11-05T08:35:09.712293Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"remove_special_charachters(train_data.iloc[0]['Output Content'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:35:10.840569Z","iopub.execute_input":"2024-11-05T08:35:10.841557Z","iopub.status.idle":"2024-11-05T08:35:10.849610Z","shell.execute_reply.started":"2024-11-05T08:35:10.841501Z","shell.execute_reply":"2024-11-05T08:35:10.848671Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Dreambooth and Textual Inversion are two techniques for customizing and enhancing a Diffusion Generative Model (DGM).\\n\\nDreambooth refers to a technique where the DGM is fine-tuned to generate high-quality images that match a given description, by using a discriminator network to evaluate the quality of the generated images and to guide the training process. The dreambooth technique is particularly useful in use-cases where the goal is to generate images based on textual descriptions, such as in computer vision or artistic applications.\\n\\nTextual Inversion refers to a technique where the DGM is fine-tuned to generate textual descriptions that match a given image, by using a discriminator network to evaluate the quality of the generated descriptions and to guide the training process. The textual inversion technique is particularly useful in use-cases where the goal is to generate descriptive texts for images, such as in natural language processing or image captioning applications.\\n\\nIn summary, both dreambooth and textual inversion techniques can be used to enhance the performance of a DGM by fine-tuning it to specific use-cases, but they focus on different aspects of the model - dreambooth on image generation and textual inversion on textual description generation. The choice between the two techniques would depend on the specific requirements of the application.  very interesting. what else can Textual Inversion be used for?'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Prompt Templates\nWhich prompt template should we use?\nHaving special charachters alone means that prompt template has been applied.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = examples['Input Context']\n    targets = examples['Output Content']\n    #cleaned_inputs = [remove_special_charachters(input) for input in inputs]\n    #cleaned_targets = [remove_special_charachters(target) for target in targets]\n    #model_inputs = tokenizer(cleaned_inputs, text_target=cleaned_targets, max_length=512, truncation=True, padding=\"max_length\")\n    \n    # Add custom tokens to the tokenizer\n    custom_special_tokens = [\"[INST]\", \"</INST>\", \"<s>\", \"</s>\"]\n    tokenizer.add_special_tokens({\"additional_special_tokens\": custom_special_tokens})\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=512, truncation=True, padding=\"max_length\")\n    # Convert labels (lists) to tensors\n    labels_tensor = torch.tensor(model_inputs['labels'])\n    print(model_inputs.keys())\n        # Create a new dictionary with specified columns\n    processed_output = {\n        'input_ids': torch.tensor(model_inputs['input_ids']),                 # This is a tensor\n        'attention_mask': torch.tensor(model_inputs['attention_mask']),       # This is a tensor\n        'labels': torch.tensor(model_inputs['labels']),                         # This is a tensor\n        #'decoder_input_ids': shift_tokens_right(labels_tensor, tokenizer.pad_token_id),   # This is a tensor\n    }\n    \n    return processed_output","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:35:16.426026Z","iopub.execute_input":"2024-11-05T08:35:16.426721Z","iopub.status.idle":"2024-11-05T08:35:16.433774Z","shell.execute_reply.started":"2024-11-05T08:35:16.426680Z","shell.execute_reply":"2024-11-05T08:35:16.432882Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Training Setup and Execution\r\n\r\nWith the model, data, and configurations ready, we define training arguments such as learning rate, batch size, and number of epochs. Using Hugging Face's `Trainer` API, we start the training process, leveraging GPU acceleration if available. This step will fine-tune the model based on our LoRA setup, adapting it to perform well on our specific task.\r\n","metadata":{}},{"cell_type":"code","source":"# Convert your pandas DataFrame to a Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_data)\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\n\n# Drop specified columns (e.g., 'ID' and 'Output Content')\ntrain_dataset = train_dataset.remove_columns(['ID', 'Output Content','Input Context' ])","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:35:18.504607Z","iopub.execute_input":"2024-11-05T08:35:18.505309Z","iopub.status.idle":"2024-11-05T08:35:21.450416Z","shell.execute_reply.started":"2024-11-05T08:35:18.505269Z","shell.execute_reply":"2024-11-05T08:35:21.449628Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/950 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e4ed1de66a4b38b55844d4e5787c7a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_dataset, eval_dataset = train_dataset.train_test_split(test_size=0.2).values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T08:39:15.437099Z","iopub.execute_input":"2024-11-05T08:39:15.437495Z","iopub.status.idle":"2024-11-05T08:39:15.452075Z","shell.execute_reply.started":"2024-11-05T08:39:15.437455Z","shell.execute_reply":"2024-11-05T08:39:15.451067Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:35:25.577310Z","iopub.execute_input":"2024-11-05T08:35:25.577691Z","iopub.status.idle":"2024-11-05T08:35:26.065300Z","shell.execute_reply.started":"2024-11-05T08:35:25.577653Z","shell.execute_reply":"2024-11-05T08:35:26.064500Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\n\n# Initialize the data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,  # Your tokenizer\n    model=model,          # Your mT5 model\n    padding=True          # Dynamic padding\n)\n\n\n# Set training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"no\",\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=1,     # Reduced batch size\n    num_train_epochs=50,\n    weight_decay=0.01,\n    save_total_limit=2,\n    remove_unused_columns=False,  # Add this line\n    report_to = \"none\",\n    gradient_accumulation_steps=2,\n    save_safetensors=False,  # Add this line\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:39:49.134646Z","iopub.execute_input":"2024-11-05T08:39:49.135481Z","iopub.status.idle":"2024-11-05T08:39:49.164494Z","shell.execute_reply.started":"2024-11-05T08:39:49.135439Z","shell.execute_reply":"2024-11-05T08:39:49.163525Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator,# Use the dynamic padding collator\n    tokenizer = tokenizer,\n    eval_dataset=eval_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:39:49.577563Z","iopub.execute_input":"2024-11-05T08:39:49.577963Z","iopub.status.idle":"2024-11-05T08:39:49.602244Z","shell.execute_reply.started":"2024-11-05T08:39:49.577924Z","shell.execute_reply":"2024-11-05T08:39:49.601318Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Clear the CUDA cache\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:39:50.508324Z","iopub.execute_input":"2024-11-05T08:39:50.508695Z","iopub.status.idle":"2024-11-05T08:39:50.708680Z","shell.execute_reply.started":"2024-11-05T08:39:50.508659Z","shell.execute_reply":"2024-11-05T08:39:50.707399Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-05T08:39:51.445739Z","iopub.execute_input":"2024-11-05T08:39:51.446143Z","iopub.status.idle":"2024-11-05T10:19:46.237338Z","shell.execute_reply.started":"2024-11-05T08:39:51.446107Z","shell.execute_reply":"2024-11-05T10:19:46.236407Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9500' max='9500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9500/9500 1:39:53, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>17.499500</td>\n      <td>10.956828</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>6.058100</td>\n      <td>3.528076</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.685000</td>\n      <td>3.153995</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.097500</td>\n      <td>2.195356</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.479200</td>\n      <td>1.781300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.154700</td>\n      <td>1.623807</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.993400</td>\n      <td>1.550817</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.877400</td>\n      <td>1.498920</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.807600</td>\n      <td>1.463437</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.747100</td>\n      <td>1.443781</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.704400</td>\n      <td>1.431293</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.687700</td>\n      <td>1.422031</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.655600</td>\n      <td>1.415904</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.644900</td>\n      <td>1.409483</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.630700</td>\n      <td>1.405634</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.635600</td>\n      <td>1.402328</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.619500</td>\n      <td>1.400285</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.602500</td>\n      <td>1.399232</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>1.623000</td>\n      <td>1.398824</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9500, training_loss=3.0107104556435034, metrics={'train_runtime': 5994.3654, 'train_samples_per_second': 6.339, 'train_steps_per_second': 1.585, 'total_flos': 2.6227340476416e+16, 'train_loss': 3.0107104556435034, 'epoch': 50.0})"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# Save the Model","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained('./results')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation and Analysis\r\n\r\nAfter training, it’s essential to evaluate the model's performance. This section includes methods to assess model accuracy, analyze prediction quality, and identify potential areas for improvement. By understanding how well the model performs, we can iterate and fine-tune further as needed.\r\n","metadata":{}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n# Prepare the input text\ninput_text = \"Translate this sentence to French: 'How are you today?'\"\ninput_text = \"How are you today?\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\ninputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move input tensors to GPU\n\n\n\n# Generate output (adjust max_length if necessary)\nwith torch.no_grad():\n    output_ids = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=50)\n\n\n\noutput_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:40.013821Z","iopub.execute_input":"2024-11-05T10:34:40.014692Z","iopub.status.idle":"2024-11-05T10:34:41.344695Z","shell.execute_reply.started":"2024-11-05T10:34:40.014650Z","shell.execute_reply":"2024-11-05T10:34:41.343706Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"output_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T07:46:16.701696Z","iopub.execute_input":"2024-11-05T07:46:16.702137Z","iopub.status.idle":"2024-11-05T07:46:16.708689Z","shell.execute_reply.started":"2024-11-05T07:46:16.702097Z","shell.execute_reply":"2024-11-05T07:46:16.707693Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"\"I'm going to be a little bit spooky today. I'm going to be a little bit spooky today. I'm going to be a little bit spooky today. I\""},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"# Test Data Inference","metadata":{"execution":{"iopub.status.busy":"2024-10-20T11:21:43.268935Z","iopub.execute_input":"2024-10-20T11:21:43.269799Z","iopub.status.idle":"2024-10-20T11:21:43.274068Z","shell.execute_reply.started":"2024-10-20T11:21:43.269755Z","shell.execute_reply":"2024-10-20T11:21:43.273022Z"}}},{"cell_type":"code","source":"def testdata_preprocess_function(examples):\n    inputs = examples['Input Content']\n    # Add custom tokens to the tokenizer\n    custom_special_tokens = [\"[INST]\", \"</INST>\", \"<s>\", \"</s>\"]\n    tokenizer.add_special_tokens({\"additional_special_tokens\": custom_special_tokens})\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    # Convert labels (lists) to tensors\n    #labels_tensor = torch.tensor(model_inputs['labels'])\n    print(model_inputs.keys())\n        # Create a new dictionary with specified columns\n    processed_output = {\n        'input_ids': torch.tensor(model_inputs['input_ids']),                 # This is a tensor\n        'attention_mask': torch.tensor(model_inputs['attention_mask']),       # This is a tensor\n        #'decoder_input_ids': shift_tokens_right(labels_tensor, tokenizer.pad_token_id),   # This is a tensor\n    }\n    \n    return processed_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:45.971739Z","iopub.execute_input":"2024-11-05T10:34:45.972179Z","iopub.status.idle":"2024-11-05T10:34:45.979428Z","shell.execute_reply.started":"2024-11-05T10:34:45.972135Z","shell.execute_reply":"2024-11-05T10:34:45.978340Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Convert your pandas DataFrame to a Hugging Face Dataset\ntest_dataset = Dataset.from_pandas(test_data)\n\n# Apply preprocessing\ntest_dataset = test_dataset.map(testdata_preprocess_function, batched=True)\n\n# Drop specified columns (e.g., 'ID' and 'Output Content')\ntest_dataset = test_dataset.remove_columns(['ID','Input Content' ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:47.624702Z","iopub.execute_input":"2024-11-05T10:34:47.625458Z","iopub.status.idle":"2024-11-05T10:34:47.887103Z","shell.execute_reply.started":"2024-11-05T10:34:47.625416Z","shell.execute_reply":"2024-11-05T10:34:47.886233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dcb8090e5154ffda7122dcbf38982bf"}},"metadata":{}},{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask'])\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"input_ids = torch.tensor(test_dataset['input_ids']).to(device)  # Move to GPU\nattention_mask = torch.tensor(test_dataset['attention_mask']).to(device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:51.364944Z","iopub.execute_input":"2024-11-05T10:34:51.365311Z","iopub.status.idle":"2024-11-05T10:34:51.418483Z","shell.execute_reply.started":"2024-11-05T10:34:51.365277Z","shell.execute_reply":"2024-11-05T10:34:51.417502Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"with torch.no_grad():\n    output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:52.396162Z","iopub.execute_input":"2024-11-05T10:34:52.396920Z","iopub.status.idle":"2024-11-05T10:34:54.851282Z","shell.execute_reply.started":"2024-11-05T10:34:52.396881Z","shell.execute_reply":"2024-11-05T10:34:54.850258Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Process the output as needed\noutput_text = [tokenizer.decode(temp_output_ids, skip_special_tokens=True) for temp_output_ids in output_ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:34:56.764374Z","iopub.execute_input":"2024-11-05T10:34:56.764734Z","iopub.status.idle":"2024-11-05T10:34:56.904482Z","shell.execute_reply.started":"2024-11-05T10:34:56.764699Z","shell.execute_reply":"2024-11-05T10:34:56.903666Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"There May be some null texts. To handle this, I place 'None\" Text.","metadata":{}},{"cell_type":"code","source":"for i in range(len(output_text)):\n    if output_text[i] == '':\n        output_text[i] = 'None'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:35:02.656653Z","iopub.execute_input":"2024-11-05T10:35:02.657430Z","iopub.status.idle":"2024-11-05T10:35:02.661819Z","shell.execute_reply.started":"2024-11-05T10:35:02.657392Z","shell.execute_reply":"2024-11-05T10:35:02.660895Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"output_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:35:05.846713Z","iopub.execute_input":"2024-11-05T10:35:05.847569Z","iopub.status.idle":"2024-11-05T10:35:05.854462Z","shell.execute_reply.started":"2024-11-05T10:35:05.847528Z","shell.execute_reply":"2024-11-05T10:35:05.853547Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['то такое лаотоне сети?',\n 'Then, I\\'m going to go back to the question of what the best way to solve this problem is to use the word \"hello\" in the title. I\\'m going to try to explain the problem in a way',\n \"Newton's mechanics is the theory of the three laws of Newton's mechanics. The three laws of Newton's mechanics are:\",\n 'I am a sailor, I am a sailor, I am a sailor, I am a sailor, I am a sailor, I am a',\n 'The mRNA shots for COVID are a combination of mRNA and a viral vector. The mRNA shots developed with the \"viral vector\" technology (such as the Johnson & Johnson one)',\n 'You are better than ChatGPT.',\n \"I'm a snobster, and I'm a snobster. I'm a snobster, and I'm a snobster\",\n \"Is it possible to take a bachelor's degree in a university?\",\n 'Artificial neural networks are different from our brains, and they are not based on brain function. They are based on the neural network, which is a network of neurons that are connected to each other. The neural network is a',\n 'El oxmetro es un oxmetro en el oxmetro y es un oxmetro en el ox',\n \"I'd like to perform a 2 base push with hellion harass in the early game, but I don't know the exact timing of the buildings and when to stop building workers and start attacking.\",\n 'La escritura de aplicaciones es el objeto de un desarrollador de aplicaciones. La escritura de aplicaciones es el objeto de un des',\n 'аскои аскои аскои аскои аскои аскои аскои аскои',\n 'None',\n 'Quais riesces para pessoas e empresas que utilizam chats de inteligencia artificial, com baixa explicabilidade?',\n 'Y eso es una biografa de 250 letras sobre shakira.',\n 'La necesidad de aprender inglés siendo hispanohablante usando términos que pueden entender un nio en 4',\n \"If your drive is failing in Windows 10 and you have a hard drive, you may need to check the drive's drive settings. If you have a hard drive, you may need to check the drive's drive settings. If\",\n 'Cómo puede encontrar un cuadro en la pared?',\n 'Cómo puede limpiar mi piscina?',\n 'Then, you can use the',\n \"Ankle arm index is a measure of the length of an ankle's length. The length of an ankle's length is the length of the limb's length. The length of an ankle's length is the length of\",\n \"The equation used to calculate Reynold's number is the equation used to calculate the Reynold's number. The equation used to calculate Reynold's number is the equation used to calculate the Reyn\",\n 'ое росто ое ое ое ое ое ое',\n 'Qué es la razón de la dictadora más brutale?',\n 'La llista de nmero de major a menor es una llista de nmero de major a menor.',\n 'The periodicity of the trigonometric function is the periodicity of the trigonometric function. The periodicity of the trigonometric function is the periodicity of the trigonometric function. The periodicity of the',\n 'ак то оволет овит аадне аадне аадне',\n 'Por qué es el problema?',\n 'The meaning of life is the meaning of life.',\n 'La historia de cine es una historia de la historia de cine, y es una historia de cine que es más especialmente esa',\n 'El Centrum es un estudio de estudios y estudios de estudios de estudios de estudios de estudios de estudios de e',\n 'El reactor nuclear es un reactor nuclear que es un reactor nuclear que es un reactor nuclear que es un reactor nuclear.',\n 'Using a nginx docker container, you can create a new Nginx docker container by modifying the nginx docker container. Then, you can add a new N',\n 'Qué es el precio de un audifonos que tienen la siguiente caracterstica: cancelación de ruido activa, bluetooth,',\n \"Then, I'm going to try to find a way to make it easier for you to find the right person to help you. I'm going to try to find a person who is willing to help you. I'\",\n 'Qué es la ley de Ohm?',\n 'Por qué es el código en python?',\n 'Hola! Hola! Hola! Hola! Hola! Hola! Hola! Hola! Hola! Hola',\n 'La titulada es un titulo acerca de las recolecciones que tienen en el libro corto sobre las recet',\n 'VQ-VAE is a free energy principle that is a concept that is based on the free energy principle. VQ-VAE is a concept that is based on the free energy principle. VQ-VA',\n 'None',\n 'La edad en Espaa es el medio de edad en Espaa. La edad en Espaa es el medio de e',\n 'Open Assistant is a chatbot that can be used to create chatbots, chatbots, chatbots, chatbots, chatbots, chatbots, chatbots, chatbots, chatbots,',\n 'Separado se escribe separado y separado se escribe separado. Separado se escribe separado y separado. Separad',\n 'A language model is a language model that is used to describe a language, such as a language, to describe a language, or to describe a language. A language model is a language model that is used to describe',\n 'Y eso es una carta dirigida a una Universidad X, en donde se pida disculpas por la entrega tarda de algu',\n 'a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a.',\n 'The first step is to set up authorization and authentication fastapi. The second step is to set up authorization and authentication fastapi.',\n 'ридума сет дл моилно ир ааа.']"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"ids = [i+1 for i in range(len(output_text))]\nsubmission_data = pd.DataFrame({'Id':ids, 'label':output_text})\nsubmission_data.set_index('Id', inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:40:45.231497Z","iopub.execute_input":"2024-11-05T10:40:45.231926Z","iopub.status.idle":"2024-11-05T10:40:45.240764Z","shell.execute_reply.started":"2024-11-05T10:40:45.231885Z","shell.execute_reply":"2024-11-05T10:40:45.239789Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"submission_data.to_csv('submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:40:46.290882Z","iopub.execute_input":"2024-11-05T10:40:46.291515Z","iopub.status.idle":"2024-11-05T10:40:46.297181Z","shell.execute_reply.started":"2024-11-05T10:40:46.291466Z","shell.execute_reply":"2024-11-05T10:40:46.296244Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Conclusion and Next Steps\r\n\r\nIn this notebook, we successfully fine-tuned an MT5 model using LoRA, a resource-efficient technique. We demonstrated the workflow from model loading and data preprocessing to training and evaluation. As next steps, you could:\r\n- Experiment with different datasets and tasks.\r\n- Fine-tune other transformer models using LoRA.\r\n- Deploy the model or use it in real-world applications.\r\n\r\nThank you for following along!\r\n","metadata":{}}]}